{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "img[alt=pic] { width: 400px; }\n",
       "img[alt=pic-big] { width: 700px; }\n",
       "img[alt=formula] { width: 200px; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<style>\n",
    "img[alt=pic] { width: 400px; }\n",
    "img[alt=pic-big] { width: 700px; }\n",
    "img[alt=formula] { width: 200px; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn image classification 2\n",
    "Continuing on our [previous workshop](https://github.com/xinbinhuang/learn_image_classification). \n",
    "Today we are going to dive into the theories behind CNN and image classification.\n",
    "\n",
    "## Welcoming and Intro\n",
    "### [Shool of AI](https://www.theschool.ai/)\n",
    "Our mission is to offer a world-classe AI education to anyone on Earth for free.\n",
    "\n",
    "**Today's agenda:**\n",
    "- Winning submission of the previous code chanllenge - Guru\n",
    "- Edge and Shape Detection with covolution\n",
    "- Theory behind neural network (i.e. CNN)\n",
    "- Live coding - building a model with Keras\n",
    "- Other applications\n",
    "- Coding chanllenge \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional filters\n",
    "\n",
    "* Finding the edges in the images by apllying filters which can detect straight edges, curves etc across RGB channels.\n",
    "\n",
    "![](../additional/img/convolution-filter-example.png)\n",
    "\n",
    "## Max Pooling\n",
    "\n",
    "* Also called the down-sampling layer.\n",
    "* Reducing dimensionality and allowing for assumptions to be made about features\n",
    "\n",
    "![pic](../additional/img/maxpool.jpg)\n",
    "\n",
    "\n",
    "## ReLU\n",
    "> Rectified Linear Units\n",
    "\n",
    "* An activation function f(x) = Max(0, x) which can be used by neurons\n",
    "* activation function is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1  or 1 to -1 etc.\n",
    "* Is used instead of logistic function to set negative values (gradient) to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network and fully connected layer/ classification\n",
    "\n",
    "\n",
    "### A simple Neural network with 1 hidden layer\n",
    "\n",
    "![pic](https://ml.berkeley.edu/blog/assets/tutorials/3/neural%20network.png)\n",
    "\n",
    "\n",
    "**What happens in each neuron?**\n",
    "\n",
    "**Main operation:**  \n",
    "multiplying each input by a different weight, adding them all together, adding an additional number called the bias, and applying an activation function. \n",
    "\n",
    "Namely:\n",
    "\n",
    "$$y_i = f(\\sum_ix_i \\cdot w_i + b_i)$$\n",
    "\n",
    "![pic](https://ml.berkeley.edu/blog/assets/tutorials/3/neuron.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "Some properties\n",
    "- Smooth (differentiable)\n",
    "- Monotonic\n",
    "- non-linear *\n",
    "\n",
    "*: Why non-linear? \n",
    "- to capture interesting patterns between the dependent and independent variables.\n",
    "\n",
    "Some common activation functions for hidden layers:\n",
    "- **Sigmoid**\n",
    "- **Tanh**\n",
    "- **Relu**\n",
    "- ...\n",
    "\n",
    "What do they look like\n",
    "\n",
    "![pic-big](https://camo.githubusercontent.com/a34bdfceac549f6a400269e1dc88bed68684832e/687474703a2f2f6164696c6d6f756a616869642e636f6d2f696d616765732f61637469766174696f6e2e706e67)\n",
    "\n",
    "Some comparison on speed of different [*Activation functions*](http://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,5,2&seed=0.16034&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "\n",
    "#### Last layer for classification:\n",
    "\n",
    "**Softmax**  \n",
    "\n",
    "Formula: \n",
    "![formula](https://cloud.githubusercontent.com/assets/14886380/22743247/9eb7c856-ee54-11e6-98ca-a7e03120b1f8.png)\n",
    "\n",
    "Transfer real-valued output to probabilities.\n",
    "\n",
    "![pic](https://cloud.githubusercontent.com/assets/14886380/22743228/91be8964-ee54-11e6-9a59-c55ea02a9146.png)\n",
    "\n",
    "\n",
    "#### Architecture - Layer Patterns\n",
    "The most common ConvNet architecture:  \n",
    "`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`\n",
    "\n",
    "Example :     \n",
    "\n",
    "![pic-big](http://cs231n.github.io/assets/cnn/convnet.jpeg)\n",
    "\n",
    "## Training\n",
    "\n",
    "General steps for training:\n",
    "- **Forwardpropagation**:\n",
    "    - Calculate the outputs with the iputs and weights\n",
    "    - Calculate the cost with the outputs and the actual labels\n",
    "- **Backpropagation**: \n",
    "    - calculate the derivatives of all weights in all the layers using **chain rule**\n",
    "- Use **Gradient descent** to update the weights\n",
    "\n",
    "### Cost function\n",
    "To train a model and update the weights for each layers, we need to specify a **cost function** that quantifies the performance of your model by outputting large values for very wrong answers and small values for more correct answers.\n",
    "\n",
    "**Cross-entropy**\n",
    "- common cost function for classification  \n",
    "![pic](https://cdn-images-1.medium.com/max/1000/0*YRAt7P06fL7TObX-.png)\n",
    "\n",
    "- penalize for confident and wrong classification  \n",
    "![pic](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
    "\n",
    "### Backpropagation\n",
    "- Goal: find the derivatives of the cost function with respect to all of the weights and biases in a neural network\n",
    "\n",
    "In mathematical notation:\n",
    "$$\\frac{\\partial J}{\\partial w_i}, \\frac{\\partial J}{\\partial b_i}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $J$ : is the cost function\n",
    "- $w_i$, $b_i$ : are the weight and bias in any neurons and any layers \n",
    "\n",
    "**How**:\n",
    "- Find the gradient of the last layer first, and then the second to last layer, and end up working backward to the input layer.\n",
    "    - using **chain rule**\n",
    "\n",
    "![pic](https://ml.berkeley.edu/blog/assets/tutorials/3/backprop%20slope.png)\n",
    "\n",
    "### Optimization - how to update the weight\n",
    "\n",
    "### Gradient Descent\n",
    "Use gradient descent to minimize the cost function by updatinig model parameters.\n",
    "\n",
    "$$weight = weight - learning\\ rate \\times gradient^*$$\n",
    "\n",
    "*: gradient of the cost function with respect to the weights\n",
    "\n",
    "![pic](https://cdn-images-1.medium.com/max/1000/0*rBQI7uBhBKE8KT-X.png)\n",
    "\n",
    "\n",
    "The hyperparameter affecting your speed to update your weights is **learning rate**\n",
    "\n",
    "### Learning rate\n",
    "- Your choice of learning rate may highly affect your model\n",
    "    - too low: too long to train\n",
    "    - too high: never converge\n",
    "![pic-big](https://cdn-images-1.medium.com/max/1000/0*QwE8M4MupSdqA3M4.png)\n",
    "- Choose your learning rate by inspecting the reationship between the number of iterations (x-axes) and the cost-function (y-axes).\n",
    "![pic-big](https://cdn-images-1.medium.com/max/1000/0*VrzWuzdNNCfJsDiG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live coding with Keras\n",
    "We will use Google CoLab and Keras to build a CNN. The notebook is [here](../keras_collab_demo/Image_Classification_II_Demo.ipynb)\n",
    "\n",
    "**Google CoLab**  \n",
    "Google Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud. They also provide free GPU setting.\n",
    "\n",
    "**Keras**  \n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
